# 第九章 并发编程

## 9.1 操作系统发展史

1. 人机矛盾：CPU利用率低
2. 磁带存储 + 批处理
   - 降低数据的读取时间
   - 提高cpu的利用率
3. 多道操作系统  —— 在一个任务遇到IO的时候主动让出CPU
   - 数据隔离
   - 时空复用：能够在一个任务遇到io操作的时候主动把cpu让出来，给其他的任务使用
     - 切换要不要占用时间 ：要占用
     - 切换这件事是谁做的 ：操作系统
4. 分时操作系统 —— 给时间分片，让多个任务轮流使用CPU
   - 时间分片
     - cpu的轮转
     - 每一个程序分配一个时间片
   - 特点
     - 要切换 要占用时间
     - 反而降低了cpu的利用率
     - 提高了用户体验
5. 分时操作系统 + 多道操作系统 + 实时操作系统
   - 多个程序一起在计算机中执行
   - 一个程序如果遇到IO操作，切出去让出CPU
   - 一个程序没有遇到IO，但是时间片到时了，切出去让出CPU
6. 网络操作系统
7. 分布式操作系统

## 9.2 进程

### 9.2.1 基本概念

1. 定义：运行中的程序

2. 程序和进程之间的区别：

   - 程序只是一个文件
   - 进程是这个文件被CPU运行起来了

3. 进程是计算机中最小的资源分配单位

4. 在操作系统中的唯一标识符 ：pid

5. 进程调度：

   - 操作系统调度进程的算法
     - 短作业优先算法
     - 先来先服务算法
     - 时间片轮转算法
     - 多级反馈算法

6. 并行与并发

   - 并行
     - 两个程序 两个CPU 每个程序分别占用一个CPU自己执行自己的
     - 看起来是同时执行，实际在每一个时间点上都在各自执行着
   - 并发
     - 两个程序 一个cpu 每个程序交替的在一个cpu上执行
     - 看起来在同时执行，但是实际上仍然是串行

7. 同步 / 异步 / 阻塞 / 非阻塞

   - 同步：一个任务的完成需要依赖另外一个任务时，只有等待被依赖的任务完成后，依赖的任务才能算完成，这是一种可靠的任务序列，要么成功都成功，失败都失败，两个任务的状态可以保持一致
   - 异步：不需要等待被依赖的任务完成，只是通知被依赖的任务要完成什么工作，依赖的任务也立即执行，只要自己完成了整个任务就算完成了，至于被依赖的任务最终是否真正完成，依赖它的任务无法确定，所以它是不可靠的任务序列
   - 阻塞：cpu不工作
   - 非阻塞：cpu工作
   - 同步阻塞
     - conn.recv
     - socket 阻塞的tcp协议的时候
   - 同步非阻塞
     - func()  没有io操作
     - socket  非阻塞的tcp协议的时候
     - 调用函数（这个函数内部不存在io操作）
   - 异步非阻塞
     - 把func扔到其他任务里去执行了
     - 我本身的任务和func任务各自执行各自的 没有io操作
   - 异步阻塞

8. 进程的三状态

   - 就绪(Ready)状态

     - 当进程已分配到除CPU以外的所有必要的资源，只要获得处理机便立即执行，这时的进程状态称为就绪状态

   - 执行/运行（Running）

     - 状态当进程已获得处理机，其程序正在处理机上执行，此时的进程状态称为执行状态

   - 阻塞(Blocked)状态

     - 正在执行的进程，由于等待某个事件发生而无法执行时，便放弃处理机而处于阻塞状态。引起进程阻塞的事件可有多种，例如，等待I/O完成、申请缓冲区不能满足、等待信件(信号)等

   - 进程的三状态图

     ![1557819282244](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\1557819282244.png)

9. 子进程 / 父进程 

   - 在父进程中创建子进程
     - 在pycharm中启动的所有py程序都是pycharm的子进程
   - os模块
     - os.getpid()：获取子进程的pid
     - os.getppid()：获取父进程的pid

### 9.2.2 进程模块

1. 开启进程

   - 导入模块：from multiprocessing import Process

   ```python
   # 面向函数方式：
   import os
   import time
   from multiprocessing import Process
   
   def eat():
       print('start eating',os.getpid())
       time.sleep(1)
       print('end eating',os.getpid())
   
   def sleep():
       print('start sleeping',os.getpid())
       time.sleep(1)
       print('end sleeping',os.getpid())
   
   if __name__ == '__main__':
       p1 = Process(target=eat)    # 创建一个即将要执行eat函数的进程对象
       p1.start()                  # 异步 调用开启进程的方法 但是并不等待这个进程真的开启
       p2 = Process(target=sleep)  # 创建一个即将要执行sleep函数的进程对象
       p2.start()                  # 异步 调用开启进程的方法 但是并不等待这个进程真的开启
       print('main :',os.getpid())
   ```

   ```python
   # 面向对象方式：
   import os
   import time
   from multiprocessing import Process
   
   class MyProcecss1(Process):
       def __init__(self,x,y):
           self.x = x
           self.y = y
           super().__init__()
       def run(self):
           print(self.x,self.y,os.getpid())
           for i in range(5):
               print('in son2')
   
   if __name__ == '__main__':
       mp = MyProcecss1(1,2)
       mp.daemon = True
       mp.start()
       print(mp.is_alive())
       time.sleep(1)
       mp.terminate()
   ```

2. 操作系统创建进程的方式不同

   - windows操作系统执行开启进程的代码
     - 实际上新的子进程需要通过import父进程的代码来完成数据的导入工作
     - 所以有一些内容我们只希望在父进程中完成，就写在if __name__ == '__main__':下面
   - ios linux操作系统创建进程 fork

3. 主进程和子进程之间的关系

   - 主进程的结束逻辑
     - 主进程的代码结束
     - 所有的子进程结束
     - 给子进程回收资源
     - 主进程结束

   ```python
   # 示例：
   import os
   import time
   from multiprocessing import Process
   def func():
       print('start',os.getpid())
       time.sleep(10)
       print('end',os.getpid())
   
   if __name__ == '__main__':
       p = Process(target=func)
       p.start()   # 异步 调用开启进程的方法 但是并不等待这个进程真的开启
       print('main :',os.getpid())
       # 主进程没结束 ：等待子进程结束
       # 主进程负责回收子进程的资源
       # 如果子进程执行结束，父进程没有回收资源，那么这个子进程会变成一个僵尸进程
   ```

4. 主进程怎么知道子进程结束了的呢？

   - 基于网络、文件
   - join方法 ：阻塞，直到子进程结束就结束

   ```python
   # 示例一：
   import time
   from multiprocessing import Process
   def send_mail():
       time.sleep(3)
       print('发送了一封邮件')
   if __name__ == '__main__':
       p = Process(target=send_mail)
       p.start()   # 异步 非阻塞
       # time.sleep(5)
       print('join start')
       p.join()    # 同步 阻塞 直到p对应的进程结束之后才结束阻塞
       print('5000封邮件已发送完毕')
       
   # 示例二：开启10个进程，给公司的5000个人发邮件，发送完邮件之后，打印一个消息“5000封邮件已发送完毕”
   import time
   import random
   from multiprocessing import Process
   def send_mail(a):
       time.sleep(random.random())
       print('发送了一封邮件',a)
   
   if __name__ == '__main__':
       l = []
       for i in range(10):
           p = Process(target=send_mail,args=(i,))
           p.start()
           l.append(p)
       print(l)
       for p in l:p.join()
       # 阻塞 直到上面的十个进程都结束
       print('5000封邮件已发送完毕')
   ```

5. 总结：

   - 如何创建一个进程对象

     - 对象和进程之间的关系
       - 进程对象和进程并没有直接的关系
       - 只是存储了一些和进程相关的内容
       - 此时此刻，操作系统还没有接到创建进程的指令

   - 如何开启一个进程

     - 通过p.start()开启了一个进程--这个方法相当于给了操作系统一条指令
     - start方法的非阻塞和异步的特点
       - 在执行开启进程这个方法的时候
       - 我们既不等待这个进程开启，也不等待操作系统给我们的响应
       - 这里只是负责通知操作系统去开启一个进程
       - 开启了一个子进程之后，主进程的代码和子进程的代码完全异步

   - 父进程和子进程

     - 为了回收子进程的资源，父进程会等待着所有的子进程结束之后才结束

   - 进程开启的过程中windows和 linux / ios之间的区别

     - windows 通过（模块导入）再一次执行父进程文件中的代码来获取父进程中的数据

       - 所以只要是不希望被子进程执行的代码，就写在if __name__ == '__main__'下
       - 因为在进行导入的时候父进程文件中的__name__ != '__main__'

     - linux/ios

       - 正常的写就可以，没有if __name__ == '__main__'这件事情了

     - 补充

       ```python
       if __name__ == '__main__':
           # 控制当这个py文件被当作脚本直接执行的时候，就执行这里面的代码
           # 当这个py文件被当作模块导入的时候，就不执行这里面的代码
           print('hello hello')
       __name__ == '__main__'  # 执行的文件就是__name__所在的文件
       __name__ == '文件名'     # __name__所在的文件被导入执行的时候
       ```

   - join方法

     - 把一个进程的结束事件封装成一个join方法
     - 执行join方法的效果就是阻塞，直到这个子进程执行结束就结束阻塞

     ```python
     # 在多个子进程中使用join
     p_l= []
     for i in range(10):
     	p = Process(target=函数名,args=(参数1,参数2))
     	p.start()
     	p_l.append(p)
     for p in p_l:p.join()
     # 所有的子进程都结束之后要执行的代码写在这里
     ```

### 9.2.3 守护进程

1. 方式：有一个参数可以把一个子进程设置为一个守护进程

   - p.daemon = True

   ```python
   # 示例：
   import time
   from multiprocessing import Process
   
   def son1(a,b):
       while True:
           print('is alive')
           time.sleep(0.5)
   
   def son2():
       for i in range(5):
           print('in son2')
           time.sleep(1)
   
   if __name__ == '__main__':
       p = Process(target=son1,args=(1,2))
       p.daemon = True
       p.start()      # 把p子进程设置成了一个守护进程
       p2 = Process(target=son2)
       p2.start()
       time.sleep(2)
   ```

2. 结束：守护进程是随着主进程的代码结束而结束的

   - 所有的子进程都必须在主进程结束之前结束，由主进程来负责回收资源

3. 应用场景：

   - 生产者消费者模型
   - 和守护线程做对比

### 9.2.4 生产者消费者模型

1. 解耦：把写在一起的大的功能分开成多个小的功能处理

   - 优点：修改 复用 可读性

2. 组成：

   - producer：生产者，生产数据
   - consumer：消费者，处理数据
   - 生产者和消费者之间的容器就是队列

3. 什么是生产者消费者模型？

   - 把一个产生数据并且处理数据的过程解耦，让生产的数据的过程和处理数据的过程达到一个工作效率上的平衡，
   - 中间的容器，在多进程中我们使用队列或者可被join的队列，做到控制数据的量
     - 当数据过剩的时候，队列的大小会控制这生产者的行为
     - 当数据严重不足的时候，队列会控制消费者的行为
     - 并且我们还可以通过定期检查队列中元素的个数来调节生产者消费者的个数
   - 比如说：一个爬虫，或者一个web程序的server端
     - 爬虫：请求网页的平均时间是0.3s，处理网页代码的时候是0.003s
       - 100倍，每启动100个线程生产数据，启动一个线程来完成处理数据
     - web程序的server端：每秒钟有6w条请求，一个服务每s中只能处理2000条
       - 先写一个web程序，只负责一件事情，就是接收请求，然后把请求放到队列中
       - 再写很多个server端，从队列中获取请求，然后处理，然后返回结果

4. 生产者消费者模型

   ```python
   import time
   import random
   from multiprocessing import Process,Queue
   
   def producer(q,name,food):
       for i in range(10):
           time.sleep(random.random())
           fd = '%s%s'%(food,i)
           q.put(fd)
           print('%s生产了一个%s'%(name,food))
   
   def consumer(q,name):
       while True:
           food = q.get()
           if not food:break
           time.sleep(random.randint(1,3))
           print('%s吃了%s'%(name,food))
   
   def cp(c_count,p_count):
       q = Queue(10)
       for i in range(c_count):
           Process(target=consumer, args=(q, 'alex')).start()
       p_l = []
       for i in range(p_count):
           p1 = Process(target=producer, args=(q, 'wusir', '泔水'))
           p1.start()
           p_l.append(p1)
       for p in p_l:p.join()
       for i in range(c_count):
           q.put(None)
   if __name__ == '__main__':
       cp(2,3)
   ```

5. 生产者消费者实现的爬虫示例

   ```python
   import re
   import requests
   from multiprocessing import Process,Queue
   
   def producer(q,url):
       response = requests.get(url)
       q.put(response.text)
   
   def consumer(q):
       while True:
           s = q.get()
           if not s:break
           com = re.compile(
               '<div class="item">.*?<div class="pic">.*?<em .*?>(?P<id>\d+).*?<span class="title">(?P<title>.*?)</span>'
               '.*?<span class="rating_num" .*?>(?P<rating_num>.*?)</span>.*?<span>(?P<comment_num>.*?)评价</span>', re.S)
           ret = com.finditer(s)
           for i in ret:
               print({
                   "id": i.group("id"),
                   "title": i.group("title"),
                   "rating_num": i.group("rating_num"),
                   "comment_num": i.group("comment_num")}
               )
   
   if __name__ == '__main__':
       count = 0
       q = Queue(3)
       p_l = []
       for i in range(10):
           url = 'https://movie.douban.com/top250?start=%s&filter='%count
           count += 25
           p = Process(target=producer,args=(q,url,))
           p.start()
           p_l.append(p)
       Process(target=consumer, args=(q,)).start()
       for p in p_l:p.join()
       q.put(None)
   ```

6. joinablequeue

   ![1557909398338](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\1557909398338.png)

   ```python
   # 示例：
   import time
   import random
   from  multiprocessing import JoinableQueue,Process
   
   def producer(q,name,food):
       for i in range(10):
           time.sleep(random.random())
           fd = '%s%s'%(food,i)
           q.put(fd)
           print('%s生产了一个%s'%(name,food))
       q.join()
   
   def consumer(q,name):
       while True:
           food = q.get()
           time.sleep(random.random())
           print('%s吃了%s'%(name,food))
           q.task_done()
   
   if __name__ == '__main__':
       jq = JoinableQueue()
       p =Process(target=producer,args=(jq,'wusir','泔水'))
       p.start()
       c = Process(target=consumer,args=(jq,'alex'))
       c.daemon = True
       c.start()
       p.join()
   ```

### 9.2.5 Process类的一些方法

1. p.start()：开启进程，异步非阻塞

2. p.terminate()：结束进程， 异步非阻塞

3. p.is_alive()：判断子进程是否还活着

4. p.join()：等待子进程的结束，同步阻塞

   ```python
   # 示例：
   import time
   from multiprocessing import Process
   
   def son1():
       while True:
           print('is alive')
           time.sleep(0.5)
   
   if __name__ == '__main__':
       p = Process(target=son1)
       p.start()      # 异步 非阻塞
       print(p.is_alive())
       time.sleep(1)
       p.terminate()   # 异步的 非阻塞
       print(p.is_alive())   # 进程还活着 因为操作系统还没来得及关闭进程
       time.sleep(0.01)
       print(p.is_alive())   # 操作系统已经响应了我们要关闭进程的需求，再去检测的时候，得到的结果是进程已经结束了
   ```

5. 总结：

   - 开启进程的方式

     ```python
     # 面向函数
     def 函数名:要在子进程中执行的代码
     p = Process(target= 函数名,args=(参数1，))
     
     # 面向对象
     class 类名(Process):
     	def __init__(self,参数1，参数2):   # 如果子进程不需要参数可以不写
     		self.a = 参数1
     		self.b = 参数2
     		super().__init__()
     	def run(self):
             # 要在子进程中执行的代码
     p = 类名(参数1，参数2)
     ```

   - Process提供的操作进程的方法

     - p.start()：开启进程，异步非阻塞
     - p.terminate()：结束进程，异步非阻塞
     - p.join()：同步阻塞
     - p.isalive()：获取当前进程的状态
     - p.daemon = True：设置为守护进程，守护进程永远在主进程的代码结束之后自动结束

### 9.2.6 锁Lock

1. 如果在一个并发的场景下，涉及到某部分内容是需要修改一些所有进程共享数据资源，需要加锁来维护数据的安全

2. 在数据安全的基础上，才考虑效率问题

3. 同步存在的意义：数据的安全性

4. 方式：

   - 在主进程中实例化 lock = Lock()
   - 把这把锁传递给子进程
   - 在子进程中 对需要加锁的代码 进行 with lock：
     - with lock相当于lock.acquire()和lock.release()

5. 应用场景：（在进程中需要加锁的场景）

   - 共享的数据资源（文件、数据库）
   - 对资源进行修改、删除操作

6. 加锁之后能够保证数据的安全性 但是也降低了程序的执行效率

   ```python
   # 示例：抢票系统
   import time
   import json
   from multiprocessing import Process,Lock
   
   def search_ticket(user):
       with open('ticket_count') as f:
           dic = json.load(f)
           print('%s查询结果  : %s张余票'%(user,dic['count']))
   
   def buy_ticket(user,lock):
       # with lock:
       # lock.acquire()   # 给这段代码加上一把锁
           time.sleep(0.02)
           with open('ticket_count') as f:
               dic = json.load(f)
           if dic['count'] > 0:
               print('%s买到票了'%(user))
               dic['count'] -= 1
           else:
               print('%s没买到票' % (user))
           time.sleep(0.02)
           with open('ticket_count','w') as f:
               json.dump(dic,f)
       # lock.release()   # 给这段代码解锁
   
   def task(user, lock):
       search_ticket(user)
       with lock:
           buy_ticket(user, lock)
   
   if __name__ == '__main__':
       lock = Lock()
       for i in range(10):
           p = Process(target=task,args=('user%s'%i,lock))
           p.start()
   ```

### 9.2.7 进程之间通信

1. 进程之间的数据隔离

   ```python
   # 示例：
   from multiprocessing import Process
   n = 100
   def func():
       global n
       n -= 1
   
   if __name__ == '__main__':
       p_l = []
       for i in range(10):
           p = Process(target=func)
           p.start()
           p_l.append(p)
       for p in p_l:p.join()
       print(n)
   ```

2. 进程之间的通信 - IPC（inter process communication）

   ```python
   # 示例：
   from multiprocessing import Queue,Process
   # 先进先出
   def func(exp,q):
       ret = eval(exp)
       q.put({ret,2,3})
       q.put(ret*2)
       q.put(ret*4)
   
   if __name__ == '__main__':
       q = Queue()
       Process(target=func,args=('1+2+3',q)).start()
       print(q.get())
       print(q.get())
       print(q.get())
   ```

   ```python
   import queue
   from multiprocessing import Queue
   q = Queue(5)
   q.put(1)
   q.put(2)
   q.put(3)
   q.put(4)
   q.put(5)             # 当队列为满的时候再向队列中放数据 队列会阻塞
   print('5555555')
   try:
       q.put_nowait(6)  # 当队列为满的时候再向队列中放数据 会报错并且会丢失数据
   except queue.Full:
       pass
   print('6666666')
   
   print(q.get())
   print(q.get())
   print(q.get())   
   print(q.get())   
   print(q.get())              # 在队列为空的时候会发生阻塞
   try:
       print(q.get_nowait())   # 在队列为空的时候 直接报错
   except queue.Empty:pass
   ```

3. 内置IPC机制：

   - Queue(队列)：进程之间数据安全
     - 天生就是数据安全的
     - 基于文件家族的socket pickle lock
   - pipe(管道)：进程之间数据不安全
     - 不安全的
     - 基于文件家族的socket pickle
     - 队列 = 管道 + 锁

4. 第三方工具（软件）提供的IPC机制：

   - redis / memcache / kafka / rabbitmq
   - 优点：
     - 并发需求
     - 高可用，多个消息中间件
     - 断电保存数据
     - 解耦

### 9.2.8 进程之间的数据共享

1. mulprocessing中有一个manager类，封装了所有和进程（数据共享、数据传递）相关的数据类型

2. 但是对于字典、列表这一类的数据操作的时候会产生数据不安全

3. 需要加锁解决问题，并且需要尽量少的使用这种方式

   ```python
   from multiprocessing import Manager,Process,Lock
   
   def func(dic,lock):
       with lock:
           dic['count'] -= 1
   
   if __name__ == '__main__':
       # m = Manager()
       with Manager() as m:
           l = Lock()
           dic = m.dict({'count':100})
           p_l = []
           for i in range(100):
               p = Process(target=func,args=(dic,l))
               p.start()
               p_l.append(p)
           for p in p_l:p.join()
           print(dic)
   ```

## 9.3 线程

### 9.3.1 基本概念

1. 定义：线程是进程中的一部分，每一个进程中至少有一个线程

2. 线程是计算机中能被CPU调度的最小单位（线程是负责执行具体代码的）

   - 进程是计算机中最小的资源分配单位（进程是负责圈资源）

3. 开销：线程的创建，也需要一些开销（一个存储局部变量的结构，记录状态）

   - 创建、销毁、切换开销远远小于进程    —— 开销小

4. 一个进程中的多个线程是可以共享这个进程的数据的  —— 数据共享

5. 线程和进程的特点及区别：

   - 进程 ：数据隔离 开销大 同时执行几段代码
   - 线程 ：数据共享 开销小 同时执行几段代码

6. 线程的理论：

   - cpython、pypy解释器，不能实现多线程利用多核
     - 锁 ：GIL 
       - 全局解释器锁
       - 由cpython解释器提供的
       - 导致了在整个python程序中，同一时间只能有一个线程被CPU执行
     - 原因：cpython解释器中特殊的垃圾回收机制
     - GIL锁导致了线程不能并行，可以并发
       - 所以使用多线程并不影响高io型的操作，只会对高计算型的程序有效率上的影响
       - 遇到高计算 ： 多进程 + 多线程         分布式
   - jpython、iron python解释器，能实现多线程利用多核

   ![1557909461883](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\1557909461883.png)

### 9.3.2 线程模块

1. 开启线程

   - 导入模块：from threading import Thread
   - multiprocessing 是完全仿照这threading的类写的

   ```python
   # 面向函数方式
   # 示例一：开启单个线程
   def func():
       print('start son thread')
       time.sleep(1)
       print('end son thread',os.getpid())
   # 启动线程 start
   Thread(target=func).start()
   print('start',os.getpid())
   time.sleep(0.5)
   print('end',os.getpid())
   
   # 示例二：开启多个子线程
   def func(i):
       print('start son thread',i)
       time.sleep(1)
       print('end son thread',i,os.getpid())
   
   for i in range(10):
       Thread(target=func,args=(i,)).start()
   print('main')
   ```

   ```python
   # 面向对象方式
   class MyThread(Thread):
       def __init__(self,i):
           self.i = i
           super().__init__()
       def run(self):
           print('start',self.i,self.ident)
           time.sleep(1)
           print('end',self.i)
   
   for i in range(10):
       t = MyThread(i)
       t.start()
       print(t.ident)
   ```

2. 主线程的结束逻辑：

   - 等待所有子线程结束之后才结束
   - 主线程如果结束了，主进程也就结束了

3. join方法：阻塞，直到子线程执行结束

   ```python
   def func(i):
       print('start son thread',i)
       time.sleep(1)
       print('end son thread',i,os.getpid())
   t_l = []
   for i in range(10):
       t = Thread(target=func,args=(i,))
       t.start()
       t_l.append(t)
   for t in t_l:t.join()
   print('子线程执行完毕')
   ```

4. 在线程中不能从主线程结束一个子线程

5. 进程和线程的效率差

   - 线程比进程的效率要高

   ```python
   def func(a,b):
       c = a+b
   import time
   from multiprocessing import Process
   from threading import Thread
   if __name__ == '__main__':
       start = time.time()
       p_l = []
       for  i in range(500):
           p = Process(target=func,args=(i,i*2))
           p.start()
           p_l.append(p)
       for p in p_l:p.join()
       print('process :',time.time() - start)
   
       start = time.time()
       p_l = []
       for i in range(500):
           p = Thread(target=func, args=(i, i * 2))
           p.start()
           p_l.append(p)
       for p in p_l: p.join()
       print('thread :',time.time() - start)
   ```

6. 线程是数据共享的，所以不要在子线程内修改全局变量

   ```python
   from threading import Thread
   n = 100
   def func():
       global n    # 不要在子线程里随便修改全局变量
       n-=1
   t_l = []
   for i in range(100):
       t = Thread(target=func)
       t_l.append(t)
       t.start()
   for t in t_l:t.join()
   print(n)
   ```

### 9.3.3 守护线程

1. 守护线程一直等到所有的非守护线程都结束之后才结束，除了守护了主线程的代码之外也会守护子线程

2. 守护线程t.daemon = True：等待所有的非守护子线程都结束之后才结束

   - 非守护线程不结束，主线程也不结束
   - 主线程结束了，主进程也结束
   - 结束顺序 ：非守护线程结束 -->主线程结束-->主进程结束--> 守护线程也结束

   ```python
   import time
   from threading import Thread
   def son1():
       while True:
           time.sleep(0.5)
           print('in son1')
   def son2():
       for i in range(5):
           time.sleep(1)
           print('in son2')
   t =Thread(target=son1)
   t.daemon = True
   t.start()
   Thread(target=son2).start()
   time.sleep(3)
   ```

### 9.3.4 threading模块中的其他类

1. current_thread：在哪个线程中被调用，就返回当前线程的对象

2. enumerate：返回当前活着的线程的对象列表

   - 活着的线程，包括主线程

3. active_count：返回当前或者的线程的个数

   ```python
   from threading import current_thread,enumerate,active_count
   def func(i):
       t = current_thread()
       print('start son thread',i,t.ident)
       time.sleep(1)
       print('end son thread',i,os.getpid())
   
   t = Thread(target=func,args=(1,))
   t.start()
   print(t.ident)
   print(current_thread().ident)   # 水性杨花 在哪一个线程里，current_thread()得到的就是这个当前线程的信息
   print(enumerate())
   print(active_count())   # =====len(enumerate())
   ```

### 9.3.5 锁

1. 即便是线程，即便有GIL，也会出现数据不安全的问题
   - 操作的是全局变量
   - 做以下操作
     - += -= *= /+ 先计算再赋值才容易出现数据不安全的问题
     - 包括 lst[0] += 1  dic['key']-=1
2. 解决方式：加锁
   - 加锁会影响程序的执行效率，但是保证了数据的安全
3. 进程和线程都有锁，可以互用吗？
   - 所有在线程中能工作的基本都不能在进程中工作
   - 在进程中能够使用的基本在线程中也可以使用
   - 所以进程锁在线程中可以使用，但线程锁不能在进程中使用

#### 9.3.5.1 互斥锁 Lock

1. 特点：一把锁不能在一个线程中连续acquire，开销小

2. 使用方式：全局就创建一个，避免死锁现象，使用完记得释放

   ```python
   from threading import Lock
   lock = Lock()
   lock.acquire()
   print('*'*20)
   lock.release()
   lock.acquire()
   print('-'*20)
   lock.release()
   ```

3. 加锁之后的单例模式

   ```python
   from threading import Lock,Thread
   
   class Singleton(object):
       __instance = None
       lock = Lock()
       def __new__(cls, *args, **kwargs):
           with cls.lock:
               if not cls.__instance:
                   cls.__instance = super().__new__(cls)
           return cls.__instance
       def __init__(self,name,age):
           self.name = name
           self.age = age
   
   def func():
       a = Singleton('alex', 84)
       print(a)
   
   for i in range(10):
       t = Thread(target=func)
       t.start()
   ```

#### 9.3.5.2 死锁现象

1. 定义：在某一些线程中出现陷入阻塞并且永远无法结束阻塞的情况就是死锁现象

2. 出现死锁现象：

   - 有多把锁，一把以上
   - 多把锁交替使用

3. 解决死锁现象：

   - 递归锁 —— 将多把互斥锁变成了一把递归锁
     - 快速解决问题
     - 效率差
     - 注意：递归锁也会发生死锁现象，多把锁交替使用的时候
   - 优化代码逻辑 —— 可以使用互斥锁 解决问题
     - 效率相对好
     - 解决问题的速度相对慢

4. 避免死锁现象：在一个线程中只有一把锁，并且每一次acquire之后都要release

5. 科学家吃面示例（死锁现象）：

   ```python
   import time
   from threading import Thread,Lock
   noodle_lock = Lock()
   fork_lock = Lock()
   def eat1(name,noodle_lock,fork_lock):
       noodle_lock.acquire()
       print('%s抢到面了'%name)
       fork_lock.acquire()
       print('%s抢到叉子了' % name)
       print('%s吃了一口面'%name)
       time.sleep(0.1)
       fork_lock.release()
       print('%s放下叉子了' % name)
       noodle_lock.release()
       print('%s放下面了' % name)
   
   def eat2(name,noodle_lock,fork_lock):
       fork_lock.acquire()
       print('%s抢到叉子了' % name)
       noodle_lock.acquire()
       print('%s抢到面了'%name)
       print('%s吃了一口面'%name)
       time.sleep(0.1)
       noodle_lock.release()
       print('%s放下面了' % name)
       fork_lock.release()
       print('%s放下叉子了' % name)
   
   lst = ['alex','wusir','taibai','yuan']
   Thread(target=eat1,args=(lst[0],noodle_lock,fork_lock)).start()
   Thread(target=eat2,args=(lst[1],noodle_lock,fork_lock)).start()
   Thread(target=eat1,args=(lst[2],noodle_lock,fork_lock)).start()
   Thread(target=eat2,args=(lst[3],noodle_lock,fork_lock)).start()
   ```

#### 9.3.5.3 递归锁 RLock

1. 特点：一把锁可以连续在一个线程中acquire多次，acquire多少次就release多少次，开销大

2. 优缺点：

   - 优点：在同一个线程中多次acquire也不会发生阻塞
   - 缺点：占用了更多资源

3. 解决问题的原理：将多把互斥锁变成一把递归锁

   - 注意：
     - 递归锁也会发生死锁现象，多把锁交替使用的时候
     - 在同一个线程中，不要创建多把递归锁，只控制在一把

4. 用递归锁解决科学家吃面示例：

   ```python
   import time
   from threading import RLock,Thread
   # noodle_lock = RLock()
   # fork_lock = RLock()
   noodle_lock = fork_lock = RLock()
   print(noodle_lock,fork_lock)
   def eat1(name,noodle_lock,fork_lock):
       noodle_lock.acquire()
       print('%s抢到面了'%name)
       fork_lock.acquire()
       print('%s抢到叉子了' % name)
       print('%s吃了一口面'%name)
       time.sleep(0.1)
       fork_lock.release()
       print('%s放下叉子了' % name)
       noodle_lock.release()
       print('%s放下面了' % name)
   
   def eat2(name,noodle_lock,fork_lock):
       fork_lock.acquire()
       print('%s抢到叉子了' % name)
       noodle_lock.acquire()
       print('%s抢到面了'%name)
       print('%s吃了一口面'%name)
       time.sleep(0.1)
       noodle_lock.release()
       print('%s放下面了' % name)
       fork_lock.release()
       print('%s放下叉子了' % name)
   
   lst = ['alex','wusir','taibai','yuan']
   Thread(target=eat1,args=(lst[0],noodle_lock,fork_lock)).start()
   Thread(target=eat2,args=(lst[1],noodle_lock,fork_lock)).start()
   Thread(target=eat1,args=(lst[2],noodle_lock,fork_lock)).start()
   Thread(target=eat2,args=(lst[3],noodle_lock,fork_lock)).start()
   ```

### 9.3.6 队列：queue模块

1. 线程之间的通信，线程是安全的，不需要加锁

2. Queue：先进先出队列，用于服务，先来先服务的思想

   ```python
   from queue import Queue  # 先进先出队列
   q = Queue(3)
   q.put(0)
   q.put(1)
   q.put(2)
   print(q.get())
   print(q.get())
   print(q.get())
   ```

3. LifoQueue：后进先出队列，用于算法

   ```python
   from queue import LifoQueue  # 后进先出队列
   # last in first out 栈
   lfq = LifoQueue(4)
   lfq.put(1)
   lfq.put(3)
   lfq.put(2)
   print(lfq.get())
   print(lfq.get())
   print(lfq.get())
   ```

4. PriorityQueue：优先级队列，用于

   - 自动的排序
   - 抢票的用户级别 100000 100001
   - 告警级别

   ```python
   from queue import PriorityQueue
   pq = PriorityQueue()
   pq.put((10,'alex'))
   pq.put((6,'wusir'))
   pq.put((20,'yuan'))
   print(pq.get())
   print(pq.get())
   print(pq.get())
   ```

## 9.4 池

1. 定义：预先的开启固定个数的进程 / 线程数，当任务来临的时候，直接提交给已经开好的进程 / 线程，让这个进程 / 线程去执行就可以了
2. 优点：节省了进程 / 线程的开启 关闭 切换都需要时间，并且减轻了操作系统调度的负担

### 9.4.1 进程池

1. 缺点：开销大，一个池中的任务个数限制了我们程序的并发个数

2. 示例：

   ```python
   # 示例一：
   import os
   import time
   import random
   from concurrent.futures import ProcessPoolExecutor
   
   def func():
       print('start',os.getpid())
       time.sleep(random.randint(1,3))
       print('end', os.getpid())
       
   if __name__ == '__main__':
       p = ProcessPoolExecutor(5)   # 个数一般是CPU的个数
       for i in range(10):
           p.submit(func)
       p.shutdown()                # 关闭池之后就不能继续提交任务，并且会阻塞，直到已经提交的任务完成
       print('main',os.getpid())
       
   # 示例二：任务的参数 + 返回值
   import os
   import time
   import random
   from concurrent.futures import ProcessPoolExecutor
   
   def func(i,name):
       print('start',os.getpid())
       time.sleep(random.randint(1,3))
       print('end', os.getpid())
       return '%s * %s'%(i,os.getpid())
   
   if __name__ == '__main__':
       p = ProcessPoolExecutor(5)
       ret_l = []
       for i in range(10):
           ret = p.submit(func,i,'alex')      # 接受一个future对象ret
           ret_l.append(ret)
       for ret in ret_l:
           print('ret-->',ret.result())        # 使用ret.result()获取返回值   同步阻塞
       print('main',os.getpid())
   ```

### 9.4.2 线程池

1. 线程池示例：

   ```python
   from concurrent.futures import ThreadPoolExecutor
   def func(i):
       print('start', os.getpid())
       time.sleep(random.randint(1,3))
       print('end', os.getpid())
       return '%s * %s'%(i,os.getpid())
   tp = ThreadPoolExecutor(20)         # 个数一般是CPU的个数的5倍
   # 获取返回值方法一：
   ret_l = []
   for i in range(10):
       ret = tp.submit(func,i)
       ret_l.append(ret)
   # tp.shutdown()    关闭池之后就不能继续提交任务，并且会阻塞，直到已经提交的任务完成
   for ret in ret_l:
       print(ret.result())
   # 获取返回值方法二：
   ret = tp.map(func,range(20))
   for i in ret:
   	print(i)
   ```

2. 回调函数示例：

   - 回调函数：add_done_callback
     - 执行完子线程任务之后直接调用对应的回调函数
     - 爬取网页，需要等待数据传输和网络上的响应高IO的，交给子线程
     - 分析网页，没有什么IO操作，这个操作没必要在子线程完成，交给回调函数

   ```python
   import requests
   from concurrent.futures import ThreadPoolExecutor
   def get_page(url):
       res = requests.get(url)
       return {'url':url,'content':res.text}
   
   def parserpage(ret):
       dic = ret.result()
       print(dic['url'])
   tp = ThreadPoolExecutor(5)
   url_lst = [
       'http://www.baidu.com',  
       'http://www.cnblogs.com', 
       'http://www.douban.com', 
       'http://www.tencent.com',
       'http://www.cnblogs.com/Eva-J/articles/8306047.html',
       'http://www.cnblogs.com/Eva-J/articles/7206498.html',
   ]
   # ret_l = []
   for url in url_lst:
       ret = tp.submit(get_page,url)
       # ret_l.append(ret)
       ret.add_done_callback(parserpage)
   # for ret in ret_l:
       # parserpage(ret.result())
   ```

### 9.4.3 总结

1. 方式：

   ```python
   # 创建一个池子
   tp = ThreadPoolExcutor(池中线程/进程的个数)
   # 异步提交任务
   ret = tp.submit(函数,参数1，参数2....)
   # 获取返回值
   ret.result()
   # 在异步的执行完所有任务之后，主线程/主进程才开始执行的代码
   tp.shutdown()   #  阻塞，直到所有的任务都执行完毕
   # map方法：迭代获取iterable中的内容，作为func的参数，让子线程来执行对应的任务
   ret = tp.map(func,iterable) 
   for i in ret:   #  每一个都是任务的返回值
   # 回调函数：要在ret对应的任务执行完毕之后，直接继续执行add_done_callback绑定的函数中的内容，并且ret的结果会作为参数返回给绑定的函数
   ret.add_done_callback(函数名)
   ```

2. 是单独开启线程进程还是池？

   - 如果只是开启一个子线程做一件事情，就可以单独开线程
   - 如果有大量的任务等待程序去做，要达到一定的并发数，开启线程池
   - 根据你程序的io操作也可以判定是用池还是不用池：
     - socket的server， 大量的阻塞io操作，  所以socketserver中没有使用池
     - 爬虫，获取数据快，所以爬虫使用池

## 9.5 协程

### 9.5.1 基本概念

1. 定义：
   - 能够在一个线程下的多个任务之间来回切换,那么每一个任务都是一个协程
   - 用户级别的,由我们自己写的python代码来控制切换的，是操作系统不可见的
2. 好处：
   - 一个线程中的阻塞都被其他的各种任务占满了
   - 让操作系统觉得这个线程很忙，尽量减少这个线程进入阻塞的状态，提高了单线程对CPU的利用率
   - 多个任务在同一线程中执行，也达到了一个并发的效果，规避了每一个任务的io操作，减少了线程的个数，减轻了操作系统的负担
3. 在cpython解释器下，协程和线程都不能利用多核，都是在一个CPU上轮流执行
   - 由于多线程本身就不能利用多核，所以即便是开启了多个线程也只能轮流在一个CPU上执行
   - 协程如果把所有任务的IO操作都规避掉,只剩下需要使用CPU的操作，就意味着协程就可以做到题高CPU利用率的效果

### 9.5.2 多线程和协程的区别

1. 线程：切换需要操作系统,开销大,操作系统不可控,给操作系统的压力大
   - 操作系统对IO操作的感知更加灵敏
2. 协程：切换需要python代码,开销小,用户操作可控,完全不会增加操作系统的压力
   - 用户级别能够对IO操作的感知比较低

### 9.5.3 协程的两种切换方式

1. python原生的底层的协程模块：asyncio模块

   - asyncio模块，是基于yield机制切换的
   - 应用
     - 爬虫，webserver框架
     - 提高网络编程的效率和并发效果
   - 语法
     - async：标识一个协程函数
     - await：后面跟着一个asyncio模块提供的io操作的函数
       - 协程函数这里要切换出去，还能保证一会儿再切回来
       - await 必须写在async函数里，async函数是协程函数
     - loop：事件循环，负责在多个任务之间进行切换
       - 所有的协程的执行 调度 都离不开这个loop

   ```python
   # 示例一：起一个任务
   import asyncio
   async def demo():   # 协程方法
       print('start')
       await asyncio.sleep(1)  # 阻塞
       print('end')
   
   loop = asyncio.get_event_loop()  # 创建一个事件循环
   loop.run_until_complete(demo())  # 把demo任务丢到事件循环中去执行
   
   # 示例二：启动多个任务,并且没有返回值
   import asyncio
   async def demo():   # 协程方法
       print('start')
       await asyncio.sleep(1)  # 阻塞
       print('end')
   
   loop = asyncio.get_event_loop()  # 创建一个事件循环
   wait_obj = asyncio.wait([demo(),demo(),demo()])
   loop.run_until_complete(wait_obj)
   
   # 示例三：启动多个任务并且有返回值，按顺序取返回值
   import asyncio
   async def demo():   # 协程方法
       print('start')
       await asyncio.sleep(1)  # 阻塞
       print('end')
       return 123
   
   loop = asyncio.get_event_loop()
   t1 = loop.create_task(demo())
   t2 = loop.create_task(demo())
   tasks = [t1,t2]
   wait_obj = asyncio.wait([t1,t2])
   loop.run_until_complete(wait_obj)
   for t in tasks:
       print(t.result())
       
   # 示例四：启动多个任务并且有返回值，谁先回来先取谁的结果
   import asyncio
   async def demo(i):   # 协程方法
       print('start')
       await asyncio.sleep(10-i)  # 阻塞
       print('end')
       return i,123
   
   async def main():
       task_l = []
       for i in range(10):
           task = asyncio.ensure_future(demo(i))
           task_l.append(task)
       for ret in asyncio.as_completed(task_l):
           res = await ret
           print(res)
   
   loop = asyncio.get_event_loop()
   loop.run_until_complete(main())
   ```

   ```python
   import asyncio
   async def get_url():
       reader,writer = await asyncio.open_connection('www.baidu.com',80)
       writer.write(b'GET / HTTP/1.1\r\nHOST:www.baidu.com\r\nConnection:close\r\n\r\n')
       all_lines = []
       async for line in reader:
           data = line.decode()
           all_lines.append(data)
       html = '\n'.join(all_lines)
       return html
   
   async def main():
       tasks = []
       for url in range(20):
           tasks.append(asyncio.ensure_future(get_url()))
       for res in asyncio.as_completed(tasks):
           result = await res
           print(result)
   
   if __name__ == '__main__':
       loop = asyncio.get_event_loop()
       loop.run_until_complete(main())     # 处理一个任务
   ```

2. C语言完成的python模块：gevent模块 

   - gevent模块，是基于greenlet切换的
   - 分辨gevent是否识别例如我们写的代码中的io操作的方法：
     - 在monkey.patch_all()之前打印一个涉及到io操作的函数地址
     - 在monkey.patch_all()之后打印一个涉及到io操作的函数地址
     - 如果两个地址一致，说明gevent没有识别这个io操作，否则，就是识别到了

   ```python
   # 示例一：
   import time
   print('-->',time.sleep)
   import gevent
   from gevent import monkey
   monkey.patch_all()
   def eat():
       print('wusir is eating')
       print('in eat: ',time.sleep)
       time.sleep(1)
       print('wusir finished eat')
   
   def sleep():
       print('小马哥 is sleeping')
       time.sleep(1)
       print('小马哥 finished sleep')
   
   g1 = gevent.spawn(eat)    # 创造一个协程任务
   g2 = gevent.spawn(sleep)  # 创造一个协程任务
   g1.join()                 # 阻塞 直到g1任务完成为止
   g2.join()                 # 阻塞 直到g1任务完成为止
   
   # 示例二：join / joinall
   import time
   import gevent
   from gevent import monkey
   monkey.patch_all()
   def eat():
       print('wusir is eating')
       time.sleep(1)
       print('wusir finished eat')
   
   def sleep():
       print('小马哥 is sleeping')
       time.sleep(1)
       print('小马哥 finished sleep')
   
   g1 = gevent.spawn(eat)    
   g2 = gevent.spawn(sleep)  
   gevent.joinall([g1,g2])   # g1.join()和g2.join()
   # 多个：
   g_l = []
   for i in range(10):
       g = gevent.spawn(eat)
       g_l.append(g)
   gevent.joinall(g_l)
   
   # 示例三：获取返回值
   import time
   import gevent
   from gevent import monkey
   monkey.patch_all()
   def eat():
       print('wusir is eating')
       time.sleep(1)
       print('wusir finished eat')
       return 'wusir***'
   
   def sleep():
       print('小马哥 is sleeping')
       time.sleep(1)
       print('小马哥 finished sleep')
       return '小马哥666'
   
   g1 = gevent.spawn(eat)
   g2 = gevent.spawn(sleep)
   gevent.joinall([g1,g2])
   print(g1.value)
   print(g2.value)
   ```

   - greenlet模块

   ```python
   import time
   from  greenlet import greenlet
   
   def eat():
       print('wusir is eating')
       time.sleep(0.5)
       g2.switch()
       print('wusir finished eat')
   
   def sleep():
       print('小马哥 is sleeping')
       time.sleep(0.5)
       print('小马哥 finished sleep')
       g1.switch()
   
   g1 = greenlet(eat)
   g2 = greenlet(sleep)
   g1.switch()
   ```

## 9.6 总结知识点

1. 操作系统
   - 计算机中所有的资源都是由操作系统分配的
   - 操作系统调度任务：时间分片、多道机制
   - 提高CPU的利用率是我们努力的指标
2. 什么是io操作？
   - i：input  
     - 向内存输入  
     - 示例：input / read / recv / recvfrom / accept / connect / close
   - o：output 
     - 从内存输出
     - 示例：print / write / send / sendto / accept / connect / close
3. 什么是GIL？
   - 全局解释器锁
   - 由cpython解释器提供的
   - 导致了一个进程中多个线程同一时刻只能有一个线程访问CPU
4. 什么是IPC？
   - 进程之间的通信机制
   - 内置的模块（基于文件）：Queue（队列）、Pipe （管道）
   - 第三方工具（基于网络）：redis / rabbitmq / kafka / memcache
     - 发挥的都是消息中间件的功能
     - 并发需求、高可用、断电保存数据、解耦
5. 进程、线程、协程的区别：
   - 进程：开销大，数据隔离，能利用多核，数据不安全，操作系统控制
   - 线程：开销中，数据共享，cpython解释器下不能利用多核，数据不安全，操作系统控制
   - 协程：开销小，数据共享，不能利用多核，数据安全，用户控制
6. 什么是生产者消费者模型？
   - 把一个产生数据并且处理数据的过程解耦，让生产数据的过程和处理数据的过程达到一个工作效率上的平衡
   - 中间的容器，在多进程中我们使用队列或者可被join的队列，做到控制数据的量
     - 当数据过剩的时候，队列的大小会控制这生产者的行为
     - 当数据严重不足的时候，队列会控制消费者的行为
     - 并且我们还可以通过定期检查队列中元素的个数来调节生产者消费者的个数
   - 比如说：一个爬虫，或者一个web程序的server端
     - 爬虫：请求网页的平均时间是0.3s，处理网页代码的时候是0.003s
       - 100倍，每启动100个线程生产数据，启动一个线程来完成处理数据
     - web程序的server端：每秒钟有6w条请求，一个服务每s中只能处理2000条
       - 先写一个web程序，只负责一件事情，就是接收请求，然后把请求放到队列中
       - 再写很多个server端，从队列中获取请求，然后处理，然后返回结果
7. 在进程、线程中都需要用到锁的概念
   - 互斥锁：在一个线程中不能连续acquire多次，效率高，产生死锁的几率大
   - 递归锁：在一个线程中可以连续acquire多次，效率低，一把锁永远不死锁
   - 死锁现象：在某一些线程中出现陷入阻塞并且永远无法结束阻塞的情况就是死锁现象
     - 出现死锁：
       - 多把锁+交替使用
       - 互斥锁在一个线程中连续acquire
     - 避免死锁：
       - 在一个线程中只有一把锁，并且每一次acquire之后都要release
   - 进程和线程都有锁，可以互用吗？
     - 所有在线程中能工作的基本都不能在进程中工作
     - 在进程中能够使用的基本在线程中也可以使用
     - 所以进程锁在线程中可以使用，但线程锁不能在进程中使用
8. 关于线程的数据安全：
   - 开启线程几乎不需要时间的，start是一个异步非阻塞方法
   - 对于整数的+=、-=、*=、/=来说，异步的多线程数据不安全，同步的话就安全了
   - 对于列表的操作，无论是异步还是同步的，都是数据安全的
9. 在哪些地方用到了线程和协程？
   - 自己用线程，协程完成爬虫任务
   - 但后来有了丰富的爬虫框架，了解到scrapy / beautyful soup / aiogttp爬虫框架
   - web框架中的并发是怎样实现的
     - 传统框架：
       - django：多线程
       - flash：邮箱选用协程，其次使用线程
     - socket server：多线程
     - 异步框架：
       - tornado / sanic底层都是协程